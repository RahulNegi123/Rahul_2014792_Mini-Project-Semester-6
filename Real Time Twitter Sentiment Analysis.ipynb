{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***INSTALLING ALL THE REQUIRED MODULES***"
      ],
      "metadata": {
        "id": "RIodcZrjM_I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy --ignore-installed\n",
        "!pip install flair"
      ],
      "metadata": {
        "id": "Lo5mU4Z_NEF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5aacd4-9df8-421c-a726-fca44cabb99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweepy\n",
            "  Downloading tweepy-4.10.0-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting requests-oauthlib<2,>=1.2.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting requests<3,>=2.27.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting oauthlib<4,>=3.2.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
            "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 13.9 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 47.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests, oauthlib, requests-oauthlib, tweepy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed certifi-2022.6.15 charset-normalizer-2.1.0 idna-3.3 oauthlib-3.2.0 requests-2.28.1 requests-oauthlib-1.3.1 tweepy-4.10.0 urllib3-1.26.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flair\n",
            "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
            "\u001b[K     |████████████████████████████████| 401 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from flair) (8.13.0)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.4 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 37.8 MB/s \n",
            "\u001b[?25hCollecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.11.0+cu113)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2022.6.2)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 51.3 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (3.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (2.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.21.6)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (2.6.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (1.3.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.8.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==4.4.0->flair) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==4.4.0->flair) (2.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
            "Building wheels for collected packages: mpld3, overrides, sqlitedict, langdetect, pptree, wikipedia-api\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=7fc6d581586db827d3e542f5c168b6689032e1a9efcda90760136a0715d87644\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=d848cf8d56d841e8fd3df6eb7470e29fcefe03e7d3d5b09e62b37b9a65b63139\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=7701acc492a2fe14feafd7a160eda2d55606f939a9ea279f17c830680ecf6bed\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=07feb1809616a0f0f1ac7df55d4fcb7cd236873fca8039afec9e9c629164786a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=7190f5184826f8f32b2f12d91d40cb3820ac3bf3fa6565959b76623d9da97d83\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=912d313036dc14c0ff9efd23664894153b2909b1ea8f74a7ff04c9178e403696\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built mpld3 overrides sqlitedict langdetect pptree wikipedia-api\n",
            "Installing collected packages: pyyaml, importlib-metadata, tokenizers, sentencepiece, py4j, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, langdetect, konoha, janome, hyperopt, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.4\n",
            "    Uninstalling importlib-metadata-4.11.4:\n",
            "      Successfully uninstalled importlib-metadata-4.11.4\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.8.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.5 pyyaml-6.0 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 tokenizers-0.12.1 transformers-4.20.1 wikipedia-api-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FOR GETTING ACCESS TO TWITTER API***"
      ],
      "metadata": {
        "id": "vna8TeWVNd7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bearer = \"AAAAAAAAAAAAAAAAAAAAALoCeQEAAAAAnmzlBbdkEcR9SOAV1N3obklts6w%3DRT0n2m2XvkHtVZFOKueLDOiax4ch2xw4HJq2OusW625EuPLwVw\"\n",
        "consumer_key = \"TzLKpD6ptPYcPlbBGTDItxGGW\"\n",
        "consumer_secret = \"QuL8sc4sSVSzP97Xag1IfRvqynmsC3O6YdBFWAS2UIL2Skjtm7\" \n",
        "access_token = \"1159447442456137728-Bj23ePEo5VIHMIdqTFpNS93SqSdFdV\"\n",
        "access_token_secret = \"eNcX0OQgQ3SWsXStDowTHCJ2KFwdzqJpmUOxMynCSSdyp\""
      ],
      "metadata": {
        "id": "yguI_gNDNtgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***IMPORTING ALL THE REQUIRED MODULES***"
      ],
      "metadata": {
        "id": "PPi4lJVVNzKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import re\n",
        "import time\n",
        "\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence"
      ],
      "metadata": {
        "id": "arZZ8-eyN-zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***CREATING A CONNECTION WITH TWITTER API***"
      ],
      "metadata": {
        "id": "TxAE_aj7OD3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = tweepy.Client(bearer, consumer_key, consumer_secret, access_token, access_token_secret)\n",
        "#Check if the connection was successful\n",
        "api.get_me()"
      ],
      "metadata": {
        "id": "YtHAG3PaOO1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabeb707-1821-4959-d422-b9ab91af688f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(data=<User id=1159447442456137728 name=Anonymous username=MSFH421>, includes={}, errors=[], meta={})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TESTING TWEEPY***"
      ],
      "metadata": {
        "id": "_gmoOqBSOecd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = api.search_recent_tweets('#india')\n",
        "\n",
        "tweets = response.data\n",
        "for tweet in tweets:\n",
        "    print(tweet.text)\n",
        "    print('******************************************************************')"
      ],
      "metadata": {
        "id": "m9ruwNgPOnyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25d9ec1-7206-427d-e2ae-2087a3137d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RT @geertwilderspvv: Please India as a friend I tell you: stop being tolerant to the intolerant. Defend Hinduism against the extremists, te…\n",
            "******************************************************************\n",
            "RT @geertwilderspvv: Please India as a friend I tell you: stop being tolerant to the intolerant. Defend Hinduism against the extremists, te…\n",
            "******************************************************************\n",
            "RT @HarisAlisic: Ur daily reminder of what Muslims in #India are living through. https://t.co/9cSrgXwfu0\n",
            "******************************************************************\n",
            "RT @TarekFatah: #KanhaiyaLal\n",
            "\n",
            "My column in @TheTorontoSun on the videotaped beheading of a #Hindu tailor by two #Muslim murderers who video…\n",
            "******************************************************************\n",
            "RT @UNHumanRights: 🇮🇳#India: We are very concerned by the arrest and detention of #WHRD @TeestaSetalvad and two ex police officers and call…\n",
            "******************************************************************\n",
            "If India and China were friends, Asia would be more developed. But they are not #india #China\n",
            "******************************************************************\n",
            "RT @TarekFatah: #KanhaiyaLal\n",
            "\n",
            "My column in @TheTorontoSun on the videotaped beheading of a #Hindu tailor by two #Muslim murderers who video…\n",
            "******************************************************************\n",
            "RT @geertwilderspvv: Hindus should be safe in India.\n",
            "It is their country, their homeland, it’s theirs!\n",
            "India is no Islamic nation. \n",
            "\n",
            "#Isupp…\n",
            "******************************************************************\n",
            "RT @KhaledBeydoun: A must watch on #India\n",
            "******************************************************************\n",
            "RT @masudakhter: Bangladesh approved $200 million ( USD) loan as part of a larger package to assist debt stricken Sri Lanka. This is a firs…\n",
            "******************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***PREPROCESSING AND CLEANING THE TEXT***"
      ],
      "metadata": {
        "id": "TzJfRWcyO7Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    # remove user handle\n",
        "    text = re.sub(\"@[\\w]*\", \"\", text)\n",
        "    # remove http links\n",
        "    text = re.sub(\"http\\S+\", \"\", text)\n",
        "    # remove digits and spl characters\n",
        "    text = re.sub(\"[^a-zA-Z#]\", \" \", text)\n",
        "    # remove rt characters\n",
        "    text = re.sub(\"rt\", \"\", text)\n",
        "    # remove additional spaces\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "BE7bKLNxO9w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TESTING NORMAL VS PRE-PROCESSED TWEET***"
      ],
      "metadata": {
        "id": "MWooUOC0PG0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet.text"
      ],
      "metadata": {
        "id": "h8gEq_t9PPqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text(tweet.text)"
      ],
      "metadata": {
        "id": "Zad5OhqfPRU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***CREATING SENTIMENT ANALYSIS FUNCTION USING PRE-TRAINED MODEL FLAIR***"
      ],
      "metadata": {
        "id": "vYOixy5IPTNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = TextClassifier.load('en-sentiment') #Set the language to English\n",
        "def get_sentiment(tweet):\n",
        "    sentence = Sentence(tweet)\n",
        "    classifier.predict(sentence)\n",
        "    return str(sentence.labels).split(\"\\'\")[3]"
      ],
      "metadata": {
        "id": "RJzfEbCbPiR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***GETTING REAL-TIME TWEETS AND ANALYSING THEM***"
      ],
      "metadata": {
        "id": "MAazk3DwP0xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    # get tweets (10 tweets)\n",
        "    tweets = api.search_recent_tweets('#india').data\n",
        "\n",
        "    for tweet in tweets:\n",
        "        original_tweet = tweet.text\n",
        "        clean_tweet = preprocess_text(original_tweet)\n",
        "        sentiment = get_sentiment(clean_tweet)\n",
        "\n",
        "        print('*****************************Tweet**********************************')\n",
        "        print(original_tweet)\n",
        "        print('*******************************************************************')\n",
        "        print('Sentiment:', sentiment)\n",
        "        time.sleep(1)\n",
        "        print('\\n\\n\\n')"
      ],
      "metadata": {
        "id": "U_lbHr46P5Rn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}